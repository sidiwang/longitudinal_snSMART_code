# BJSM method with continuous outcome

# Sidi Wang
# June, 2023

model{

  for(i in 1:overall_sample_size){
    #likelihood of stage 1 outcome
    data_effect_stageI[i] ~ dnorm(effect_stageI[i], 1/(vars^2))
    #explaining
    effect_stageI[i] <- mu[treatment_stageI[i]]
  }
  
  for(j in stage2size){
    #likelihood of stage 2 outcome
    data_effect_stageII[j] ~ dnorm(effect_stageII[j], 1/(vars^2))
    #explaining
    effect_stageII[j] <- mu[treatment_stageII[j]] + alpha + beta * (data_effect_stageI[j] - mu[treatment_stageI[j]])
  }
  
  # priors
  vars ~ dnorm(0, var_value) T(0,)
  
  # mu_P
  mu1 = c(mu_guess[1], mu_mixture[1])
  tau1 = c(var_prior[1], 0.0000001)
  pi1 = c(0.5, 0.5)
  
  mu[1] ~ dnorm(mu1[r1], tau1[r1])
  r1 ~ dcat(pi1)
  
  # mu_L
  mu2 = c(mu_guess[2], mu_mixture[2])
  tau2 = c(var_prior[2], 0.0000001)
  pi2 = c(0.5, 0.5)
  
  mu[2] ~ dnorm(mu2[r2], tau2[r2])
  r2 ~ dcat(pi2)
  
  # mu_H
  mu3 = c(mu_guess[3], mu_mixture[3])
  tau3 = c(var_prior[3], 0.0000001)
  pi3 = c(0.5, 0.5)
  
  mu[3] ~ dnorm(mu3[r3], tau3[r3])
  r3 ~ dcat(pi3)
  
  alpha ~ dnorm(0, 0.01)
  beta ~ dnorm(0, 0.01)
}